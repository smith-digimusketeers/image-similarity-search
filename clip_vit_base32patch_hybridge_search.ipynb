{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Semantic Search — Complete Notebook (Image→Image, Text→Image, Image+Text→Image)\n",
        "\n",
        "```\n",
        "A single, self-contained notebook that lets you:\n",
        "\n",
        "Image → Image search with CLIP (cosine)\n",
        "Text → Image search (cosine)\n",
        "Image + Text → Image fused search (weighted cosine)\n",
        "✅ Works on CPU or GPU. Optional FAISS index for speed.\n",
        "```"
      ],
      "metadata": {
        "id": "JW2f1A6n3dTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dep"
      ],
      "metadata": {
        "id": "7aGscRiC37Pt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tkDRy5J-3cgq"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers pillow tqdm numpy matplotlib\n",
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9tpKeDz4AqD",
        "outputId": "3554bf1c-fc95-4681-df4e-e55e63e27bf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Imports & Configuration"
      ],
      "metadata": {
        "id": "hm-beD5Z4L_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Iterable, Optional, Union\n",
        "\n",
        "import os, json, math, time, dataclasses\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "try:\n",
        "    import faiss  # optional\n",
        "    HAS_FAISS = True\n",
        "except Exception:\n",
        "    HAS_FAISS = False\n",
        "\n",
        "# Configure your paths\n",
        "PHOTOS_DIR = Path(\"/content/drive/MyDrive/digi/digi_image-similarity/Photos\") # 👈 replace\n",
        "INDEX_DIR  = Path('./index')\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = 'openai/clip-vit-base-patch32'  # Swap to a larger CLIP for better recall\n",
        "BATCH_SIZE = 64                              # Reduce if you hit OOM\n"
      ],
      "metadata": {
        "id": "nALZlv0035vT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Utilities"
      ],
      "metadata": {
        "id": "LlgwUMqI4OK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp', '.tif', '.tiff'}\n",
        "\n",
        "# List images recursively\n",
        "\n",
        "def list_images(root: Path) -> List[Path]:\n",
        "    files = []\n",
        "    for p in root.rglob('*'):\n",
        "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
        "            files.append(p)\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "# Open with EXIF orientation and convert to RGB\n",
        "\n",
        "def open_image(path: Path) -> Image.Image:\n",
        "    with Image.open(path) as im:\n",
        "        im = ImageOps.exif_transpose(im)\n",
        "        return im.convert('RGB')\n",
        "\n",
        "# Chunk an iterable\n",
        "\n",
        "def chunked(it: Iterable, size: int):\n",
        "    buf = []\n",
        "    for x in it:\n",
        "        buf.append(x)\n",
        "        if len(buf) == size:\n",
        "            yield buf; buf = []\n",
        "    if buf:\n",
        "        yield buf\n",
        "\n",
        "# Row-wise L2 normalize\n",
        "\n",
        "def normalize(a: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(a, axis=1, keepdims=True) + 1e-12\n",
        "    return a / norms"
      ],
      "metadata": {
        "id": "m2iW4jUS4Ih9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) CLIP Encoder (image + text towers)\n"
      ],
      "metadata": {
        "id": "pgt8IVyu4R5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClipEncoder:\n",
        "    def __init__(self, model_name: str = MODEL_NAME, device: Optional[str] = None):\n",
        "        self.model_name = model_name\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        with torch.no_grad():\n",
        "            t = self.processor(text=['x'], return_tensors='pt')\n",
        "            t = {k: v.to(self.device) for k, v in t.items()}\n",
        "            self.dim = int(self.model.get_text_features(**t).shape[-1])\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def encode_images(self, pil_images: List[Image.Image]) -> np.ndarray:\n",
        "        if not pil_images:\n",
        "            return np.zeros((0, self.dim), dtype=np.float32)\n",
        "        batch = self.processor(images=pil_images, return_tensors='pt')\n",
        "        feats = self.model.get_image_features(pixel_values=batch['pixel_values'].to(self.device))\n",
        "        feats = F.normalize(feats, p=2, dim=-1)\n",
        "        return feats.detach().cpu().to(torch.float32).numpy()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        t = self.processor(text=[text], return_tensors='pt', padding=True)\n",
        "        t = {k: v.to(self.device) for k, v in t.items()}\n",
        "        feats = self.model.get_text_features(**t)\n",
        "        feats = F.normalize(feats, p=2, dim=-1)\n",
        "        return feats.detach().cpu().to(torch.float32).numpy()[0]\n",
        "\n",
        "enc = ClipEncoder(MODEL_NAME)\n",
        "print('Device:', enc.model.device, '| dim =', enc.dim)\n"
      ],
      "metadata": {
        "id": "9KV2tNbR4Q8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) On‑Disk Index (embeddings + metadata)\n"
      ],
      "metadata": {
        "id": "J_c4ux-64Wq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ImageMeta:\n",
        "    path: str\n",
        "    mtime: float\n",
        "\n",
        "class DiskIndex:\n",
        "    def __init__(self, index_dir: Path):\n",
        "        self.index_dir = index_dir\n",
        "        self.emb_path = index_dir / 'embeddings.npy'\n",
        "        self.meta_path = index_dir / 'images.jsonl'\n",
        "        self.header_path = index_dir / 'header.json'\n",
        "        self.faiss_path = index_dir / 'faiss.index'\n",
        "\n",
        "    def save(self, header: dict, embeddings: np.ndarray, metas: List[ImageMeta], faiss_index=None):\n",
        "        self.index_dir.mkdir(parents=True, exist_ok=True)\n",
        "        embeddings = normalize(embeddings.astype(np.float32, copy=False))\n",
        "        np.save(self.emb_path, embeddings)\n",
        "        with open(self.meta_path, 'w', encoding='utf-8') as f:\n",
        "            for m in metas:\n",
        "                f.write(json.dumps(dataclasses.asdict(m), ensure_ascii=False) + '\\n')\n",
        "        with open(self.header_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(header, f, ensure_ascii=False, indent=2)\n",
        "        if faiss_index is not None:\n",
        "            faiss.write_index(faiss_index, str(self.faiss_path))\n",
        "\n",
        "    def load(self):\n",
        "        header = json.loads(self.header_path.read_text('utf-8'))\n",
        "        embs = np.load(self.emb_path)\n",
        "        metas = [json.loads(line) for line in self.meta_path.read_text('utf-8').splitlines()]\n",
        "        return header, embs, metas\n",
        "\n",
        "    def try_load_faiss(self):\n",
        "        if HAS_FAISS and self.faiss_path.exists():\n",
        "            return faiss.read_index(str(self.faiss_path))\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "1c2DnpPZ4T6v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Build (or rebuild) the index from your photo folder\n"
      ],
      "metadata": {
        "id": "CiAYb1Ol4bYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = list_images(PHOTOS_DIR)\n",
        "print(f'Found {len(files)} images under {PHOTOS_DIR}')\n",
        "\n",
        "metas: List[ImageMeta] = []\n",
        "all_feats: List[np.ndarray] = []\n",
        "\n",
        "for batch_paths in tqdm(list(chunked(files, BATCH_SIZE)), desc='Embedding', unit='batch'):\n",
        "    pil_batch, batch_metas = [], []\n",
        "    for p in batch_paths:\n",
        "        try:\n",
        "            pil_batch.append(open_image(p))\n",
        "            batch_metas.append(ImageMeta(path=str(p), mtime=p.stat().st_mtime))\n",
        "        except Exception as e:\n",
        "            print('[skip]', p, e)\n",
        "    if not pil_batch:\n",
        "        continue\n",
        "    feats = enc.encode_images(pil_batch)\n",
        "    all_feats.append(feats)\n",
        "    metas.extend(batch_metas)\n",
        "\n",
        "if not metas:\n",
        "    raise SystemExit('No images embedded — check PHOTOS_DIR')\n",
        "\n",
        "embeddings = np.vstack(all_feats)\n",
        "header = {\n",
        "    'model_name': MODEL_NAME,\n",
        "    'dim': int(embeddings.shape[-1]),\n",
        "    'created_at': time.time(),\n",
        "    'num_items': len(metas),\n",
        "    'device_build': str(enc.model.device),\n",
        "}\n",
        "\n",
        "index = DiskIndex(INDEX_DIR)\n",
        "index.save(header, embeddings, metas)\n",
        "print(f'Saved index → {INDEX_DIR} with {len(metas)} items.')\n"
      ],
      "metadata": {
        "id": "wEjfZBp04aOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Search — Text → Image (cosine)\n"
      ],
      "metadata": {
        "id": "_3e6_nan4fbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "def search_text(index_dir: Path, text: str, topk: int = 12, prefer_faiss: bool = True):\n",
        "    index = DiskIndex(index_dir)\n",
        "    header, embs, metas = index.load()\n",
        "    embs = normalize(embs.astype(np.float32))\n",
        "\n",
        "    q = enc.encode_text(text).astype(np.float32)  # (D,)\n",
        "\n",
        "    if prefer_faiss and HAS_FAISS:\n",
        "        fidx = index.try_load_faiss()\n",
        "        if fidx is None:\n",
        "            fidx = faiss.IndexFlatIP(header['dim'])\n",
        "            fidx.add(embs)\n",
        "        D, I = fidx.search(q[None, :], min(topk, len(embs)))\n",
        "        return [(metas[int(i)], float(d)) for i, d in zip(I[0], D[0])]\n",
        "\n",
        "    sims = embs @ q\n",
        "    idx = np.argsort(-sims)[:topk]\n",
        "    return [(metas[int(i)], float(sims[int(i)])) for i in idx]\n"
      ],
      "metadata": {
        "id": "ems8EPrO4d5N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Search — Image → Image (cosine)\n"
      ],
      "metadata": {
        "id": "dTmdzAtv4ihh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_query_image(img: Union[str, Path, Image.Image]) -> np.ndarray:\n",
        "    if isinstance(img, (str, Path)):\n",
        "        pil = open_image(Path(img))\n",
        "    else:\n",
        "        pil = img\n",
        "    return enc.encode_images([pil])[0].astype(np.float32)\n",
        "\n",
        "\n",
        "def search_image(index_dir: Path, query_image: Union[str, Path, Image.Image], topk: int = 12,\n",
        "                 prefer_faiss: bool = True, include_self: bool = False):\n",
        "    index = DiskIndex(index_dir)\n",
        "    header, embs, metas = index.load()\n",
        "    embs = normalize(embs.astype(np.float32))\n",
        "\n",
        "    q = encode_query_image(query_image)  # (D,)\n",
        "\n",
        "    if prefer_faiss and HAS_FAISS:\n",
        "        fidx = index.try_load_faiss()\n",
        "        if fidx is None:\n",
        "            fidx = faiss.IndexFlatIP(header['dim'])\n",
        "            fidx.add(embs)\n",
        "        D, I = fidx.search(q[None, :], min(topk+10, len(embs)))\n",
        "        candidates = [(metas[int(i)], float(d)) for i, d in zip(I[0], D[0]) if int(i) >= 0]\n",
        "    else:\n",
        "        sims = embs @ q\n",
        "        idx = np.argsort(-sims)[:topk+10]\n",
        "        candidates = [(metas[int(i)], float(sims[int(i)])) for i in idx]\n",
        "\n",
        "    # Optionally filter out the query image itself\n",
        "    q_path = None\n",
        "    if isinstance(query_image, (str, Path)):\n",
        "        q_path = str(Path(query_image).resolve())\n",
        "    results = []\n",
        "    for meta, score in candidates:\n",
        "        p = str(Path(meta['path']).resolve())\n",
        "        if not include_self and q_path is not None and p == q_path:\n",
        "            continue\n",
        "        results.append((meta, score))\n",
        "        if len(results) >= topk:\n",
        "            break\n",
        "    return results"
      ],
      "metadata": {
        "id": "T02uYwP44hCx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Search — Image + Text → Image (fused)\n"
      ],
      "metadata": {
        "id": "aJFTH_Un4mo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine image and text queries by a weighted sum of their embeddings.\n",
        "# q = normalize(w_img * q_img + w_txt * q_txt)\n",
        "\n",
        "def search_image_plus_text(index_dir: Path,\n",
        "                           query_image: Union[str, Path, Image.Image],\n",
        "                           text: str,\n",
        "                           w_img: float = 1.0,\n",
        "                           w_txt: float = 1.0,\n",
        "                           topk: int = 12,\n",
        "                           prefer_faiss: bool = True,\n",
        "                           include_self: bool = False):\n",
        "    index = DiskIndex(index_dir)\n",
        "    header, embs, metas = index.load()\n",
        "    embs = normalize(embs.astype(np.float32))\n",
        "\n",
        "    qi = encode_query_image(query_image).astype(np.float32)\n",
        "    qt = enc.encode_text(text).astype(np.float32)\n",
        "\n",
        "    q = w_img * qi + w_txt * qt\n",
        "    q = q / (np.linalg.norm(q) + 1e-12)\n",
        "\n",
        "    if prefer_faiss and HAS_FAISS:\n",
        "        fidx = index.try_load_faiss()\n",
        "        if fidx is None:\n",
        "            fidx = faiss.IndexFlatIP(header['dim'])\n",
        "            fidx.add(embs)\n",
        "        D, I = fidx.search(q[None, :], min(topk+10, len(embs)))\n",
        "        candidates = [(metas[int(i)], float(d)) for i, d in zip(I[0], D[0]) if int(i) >= 0]\n",
        "    else:\n",
        "        sims = embs @ q\n",
        "        idx = np.argsort(-sims)[:topk+10]\n",
        "        candidates = [(metas[int(i)], float(sims[int(i)])) for i in idx]\n",
        "\n",
        "    # Optional self-filter if the image is from the index\n",
        "    q_path = None\n",
        "    if isinstance(query_image, (str, Path)):\n",
        "        q_path = str(Path(query_image).resolve())\n",
        "    results = []\n",
        "    for meta, score in candidates:\n",
        "        p = str(Path(meta['path']).resolve())\n",
        "        if not include_self and q_path is not None and p == q_path:\n",
        "            continue\n",
        "        results.append((meta, score))\n",
        "        if len(results) >= topk:\n",
        "            break\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "xODp_xSo4k7J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) Visualization helpers\n"
      ],
      "metadata": {
        "id": "dNigDe174qry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_grid(results: List[Tuple[dict, float]], cols: int = 4, max_w: int = 320, title: str = None):\n",
        "    if not results:\n",
        "        print('No results.'); return\n",
        "    rows = math.ceil(len(results)/cols)\n",
        "    if title:\n",
        "        print(title)\n",
        "    plt.figure(figsize=(cols*4, rows*4))\n",
        "    for i, (meta, score) in enumerate(results, start=1):\n",
        "        try:\n",
        "            img = open_image(Path(meta['path']))\n",
        "            img.thumbnail((max_w, max_w))\n",
        "            plt.subplot(rows, cols, i)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"{score:.3f}\\n{Path(meta['path']).name}\", fontsize=9)\n",
        "        except Exception as e:\n",
        "            print('[viz-skip]', meta['path'], e)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "def show_query(query_img_path: Union[str, Path, None] = None, text: Optional[str] = None):\n",
        "    if query_img_path is None and not text:\n",
        "        return\n",
        "    plt.figure(figsize=(4,4))\n",
        "    if query_img_path is not None:\n",
        "        img = open_image(Path(query_img_path))\n",
        "        plt.imshow(img); plt.axis('off')\n",
        "        ttl = f\"Query image: {Path(query_img_path).name}\"\n",
        "        if text:\n",
        "            ttl += f\"\\nText: {text}\"\n",
        "        plt.title(ttl)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, text, ha='center', va='center', fontsize=14)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1tMepBdt4pQS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11) Demos\n"
      ],
      "metadata": {
        "id": "lrJ_6pLX4unw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔎 Text → Image\n",
        "text_query = 'black female'\n",
        "text_hits = search_text(INDEX_DIR, text_query, topk=12)\n",
        "show_query(None, text_query)\n",
        "show_grid(text_hits, cols=4, title='Text → Image')\n"
      ],
      "metadata": {
        "id": "ALI8w1aA4tFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🖼️ Image → Image\n",
        "# Pick an existing image from your library as the query\n",
        "query_image_path = \"/content/new_cat.png\"  # 👈 replace with a specific file if you like\n",
        "img_hits = search_image(INDEX_DIR, query_image_path, topk=12, include_self=False)\n",
        "show_query(query_image_path)\n",
        "show_grid(img_hits, cols=4, title='Image → Image')\n"
      ],
      "metadata": {
        "id": "7Mngw_qJ4w40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧪 Image + Text → Image (fused)\n",
        "# Example: refine the image neighborhood with a textual intent\n",
        "fused_hits = search_image_plus_text(\n",
        "    INDEX_DIR,\n",
        "    query_image=query_image_path,\n",
        "    text='man',\n",
        "    w_img=1.0,\n",
        "    w_txt=0.8,\n",
        "    topk=12,\n",
        "    include_self=False,\n",
        ")\n",
        "show_query(query_image_path, 'man')\n",
        "show_grid(fused_hits, cols=4, title='Image + Text → Image (fused)')\n"
      ],
      "metadata": {
        "id": "OBvkus5Y5e-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWbYWJky5upC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}